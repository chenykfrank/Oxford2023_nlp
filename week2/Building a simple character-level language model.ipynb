{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDJqJ6_rfBT7"
      },
      "source": [
        "# Building a Simple Character-Level Language Model\n",
        "\n",
        "Aims:\n",
        "- Build your first language model to generate rap lyrics\n",
        "- Understand how to implement recurrent neural networks in PyTorch\n",
        "- Get familiar with PyTorch's embedding layer\n",
        "\n",
        "## What is Language Modelling?\n",
        "\n",
        "> Given a sequence of words, the language model assigns a probability to each possible word that might come next in the sequence. \n",
        "\n",
        "![](https://github.com/AI-Core/Content-Public/blob/main/Content/units/Deep%20Learning%20for%20NLP/0.%20Intro%20to%20AI%20for%20Text%20Data/3.%20Building%20a%20simple%20character-level%20language%20model/images/Language%20Model.png?raw=1)\n",
        "\n",
        "Language modeling is the process of predicting the next word in a sequence of words based on the context provided by the previous words. It is a core task in natural language processing (NLP) and is used in a wide range of applications, including speech recognition, machine translation, and chatbots.\n",
        "\n",
        "This can be used to predict the next word in a sequence, generate text that is similar to a given input, or to evaluate the quality of a translation or a summary by comparing the probability of the generated text to the probability of the original text.\n",
        "\n",
        "> It's easy to acquire data for training language models because the label is simply the next word.\n",
        "\n",
        "Language models are typically trained on large corpora of text, such as books, articles, and websites, in order to learn the statistical properties of the language and the dependencies between words. They can be implemented using various types of neural networks, such as recurrent neural networks (RNNs), long short-term memory (LSTM) networks, or transformers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gVAa96QfBT_"
      },
      "source": [
        "## Get Some Data\n",
        "\n",
        "In this example, we'll try to generate lyrics like those from your favourite artist.\n",
        "\n",
        "If you want to use your own data, you can either:\n",
        "- Copy lyrics into the code below to define your corpus (easy difficulty)\n",
        "- Create a GitHub repo and upload the lyrics there, then paste in the raw URLs, as below (intermediate difficulty)\n",
        "- Build a web-scraper to collect lyrics [like I did](https://github.com/life-efficient/Lyric-Generation/tree/main/data) (hardcore difficulty)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZqrzqNVUfBT_",
        "outputId": "d0a1c01b-277e-4d61-b414-fc3546db893d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[Intro] (One, two, three, go) [Verse 1] I don\\'t wanna run you over heavy on the gas I\\'m a million miles an hour and I\\'m never coming last Hey, hey-hey Saw you in the distance, there was something in the room Could\\'ve been the music or it could\\'ve been the moon Hey, hey-hey 00:00/15:19 10 10 Coast Contra “Never Freestyle\\' Official Lyrics & Meaning | Verified [Pre-Chorus] No, we Don\\'t Have To get emotional (Ooh-ooh, ooh-ooh) No, we (We) Don\\'t (Don\\'t) Have (Have) To get emotional [Chorus] You\\'ll never love somebody like me You\\'ll never love somеbody like me You\\'ll nevеr love, you\\'ll never love You\\'ll never love somebody like me You\\'ll never love somebody like me And you could be somebody I need You\\'ll never love, you\\'ll never love You\\'ll never love somebody like me (Hey-hey-hey) (Hey-hey-hey) [Verse 2] You don\\'t have to stay the night, only if you want I\\'m a little dark side, I\\'m a little fun Hey, hey-hey Used to like the drama, baby, I don\\'t anymore If you got some trouble, baby, leave it at the door Hey, hey-hey (Ooh-ooh, ooh-ooh)[Verse 1] Someone told me long ago There\\'s a calm before the storm I know, it\\'s been comin\\' for some time When it\\'s over, so they say It\\'ll rain a sunny day I know, shinin\\' down like water [Chorus] I wanna know, have you ever seen the rain? I wanna know, have you ever seen the rain? Comin\\' down on a sunny day 00:00/15:19 10 10 Coast Contra “Never Freestyle\\' Official Lyrics & Meaning | Verified [Verse 2] Yesterday, and days before Sun is cold and rain is hard I know, been that way for all my time \\'Til forever, on it goes Through the circle, fast and slow I know, it can\\'t stop, I wonder [Chorus] I want to know, have you ever seen the rain? I want to know, have you ever seen the rain? Comin\\' down on a sunny day Yeah! [Chorus] I want to know, have you ever seen the rain? I want to know, have you ever seen the rain? Comin\\' down on a sunny day[Verse 1: HARDY] I got turned around in some little town I\\'d never been to before Workin\\' my way through a middle of June Midnight thunderstorm There was somethin\\' in the headlights That stopped me on a dime Well, she was scared to death So I said, \"Climb in,\" and in she climbed Oh, yeah 10 10 Coast Contra “Never Freestyle\\' Official Lyrics & Meaning | Verified [Verse 2: HARDY] Well, she was bruised and broke from head to toe With a tear in her blood-stained shirt She didn\\'t tell the whole truth, but she didn\\'t have to I knew what had happenеd to her I didn\\'t load her down with questions That girl had been through еnough I just threw it in drive, looked in those eyes And asked her where he was [Chorus: Lainey Wilson, HARDY & Lainey Wilson] I don\\'t know if he\\'s angel \\'Cause angels don\\'t do what he did He was hellbent to find the man behind All the whiskey scars I hid I never thought my day of justice Would come from a judge under his seat But I knew right then I\\'d never get hit again when he said to me \"Wait in the truck Just wait in the truck\" [Verse 3: HARDY] Well, I knocked and knocked and no one came So I kicked in his double-wide door I let the hammer drop before he got To that twelve he was reachin\\' for I didn\\'t try to hide my pistol I didn\\'t even try to run I just sat on the porch, smokin\\' one of his cigarettes And waited for the cops to come[Verse 1] I hear the train a comin\\', it\\'s rolling \\'round the bend And I ain\\'t seen the sunshine since I don\\'t know when I\\'m stuck in Folsom prison, and time keeps draggin\\' on But that train keeps a rollin\\' on down to San Antone [Verse 2] When I was just a baby my mama told me, \"Son Always be a good boy, don\\'t ever play with guns\" But I shot a man in Reno just to watch him die When I hear that whistle blowing, I hang my head and cry 00:00/15:19 10 10 Coast Contra “Never Freestyle\\' Official Lyrics & Meaning | Verified [Verse 3] I bet there\\'s rich folks eating in a fancy dining car They\\'re probably drinkin\\' coffee and smoking big cigars Well I know I had it coming, I know I can\\'t be free But those people keep a-movin\\' And that\\'s what tortures me [Verse 4] Well if they freed me from this prison If that railroad train was mine I bet I\\'d move it on a little farther down the line Far from Folsom prison, that\\'s where I want to stay And I\\'d let that lonesome whistle blow my blues away[Verse 1] Love is a burning thing And it makes a fiery ring Bound by wild desire I fell into a ring of fire [Chorus] I fell into a burning ring of fire I went down, down, down And the flames went higher And it burns, burns, burns The ring of fire The ring of fire [Instrumental Break] [Chorus] I fell into a burning ring of fire I went down, down, down And the flames went higher And it burns, burns, burns The ring of fire The ring of fire [Verse 2] The taste of love is sweet When hearts like ours meet I fell for you like a child Oh, but the fire went wild [Chorus] I fell into a burning ring of fire I went down, down, down And the flames went higher And it burns, burns, burns The ring of fire The ring of fire I fell into a burning ring of fire I went down, down, down And the flames went higher And it burns, burns, burns The ring of fire The ring of fire[Intro] (Na-na-na-na-na, na-na-na-na-na) (Na-na-na-na-na, na-na-na-na-na) [Chorus] I don\\'t need your love I don\\'t need your sympathy I don\\'t need your heart I just need some sober sleep Keep me in the dark When you\\'ve been lyin\\' next to me Slippin\\' on the secrets you keep Yeah, that shit gives me the creeps 10 10 Coast Contra “Never Freestyle\\' Official Lyrics & Meaning | Verified [Post-Chorus] (La-la-la-la-la, la-la-la-la-la) (La-la-la-la-la, la-la-la-la-la) [Verse 1] You only call me when you need somethin\\' I guess that I\\'ll take it whenever I can Your beauty and desire to make nobody speak is a trap And no one likes that [Chorus] I don\\'t need your love I don\\'t need your sympathy I don\\'t need your heart I just need some sober sleep Keep me in the dark When you\\'ve been lyin\\' next to me Slippin\\' on thе secrets you keep Yeah, that shit givеs me the creeps [Post-Chorus] (La-la-la-la-la, la-la-la-la-la) That shit gives me the creeps (La-la-la-la-la, la-la-la-la-la) [Verse 2] It takes a lot for me to get freaked out But lately, I\\'ve been livin\\' in your haunted house You\\'re stunning and gorgeous But your heart is the ugliest thing That I\\'ve ever seen[Verse 1] It\\'s 4 AM I can\\'t turn my head off Wishing these memories would fade, they never do Turns out people lie They say, \"Just snap your fingers\" As if it was really that easy for me to get over you [Bridge] I just need time 00:00/15:19 10 10 Coast Contra “Never Freestyle\\' Official Lyrics & Meaning | Verified [Chorus] Snappin\\' one, two Where are you? You\\'re still in my heart Snappin\\' three, four Don\\'t need you here anymore Get out of my heart \\'Cause I might snap [Verse 2] I\\'m writin\\' a song Said, \"This is the last one\" How many last songs are left? I\\'m losing count Since June 22nd My heart\\'s been on fire I was spendin\\' my nights in the rain, tryna put it out [Chorus] So I\\'m snappin\\' one, two Where are you? You\\'re still in my heart Snappin\\' three, four Don\\'t need you here anymore Get out of my heart \\'Cause I might snap[Verse 1] I walked through the door with you, the air was cold But somethin\\' \\'bout it felt like home somehow And I left my scarf there at your sister\\'s house And you\\'ve still got it in your drawer, even now [Verse 2] Oh, your sweet disposition and my wide-eyed gaze We\\'re singin\\' in the car, getting lost upstate Autumn leaves fallin\\' down like pieces into place And I can picture it after all these days 00:00/15:19 10 10 Coast Contra “Never Freestyle\\' Official Lyrics & Meaning | Verified [Pre-Chorus] And I know it\\'s long gone and That magic\\'s not here no more And I might be okay, but I\\'m not fine at all Oh, oh, oh [Chorus] \\'Causе there we arе again on that little town street You almost ran the red \\'cause you were lookin\\' over at me Wind in my hair, I was there I remember it all too well [Verse 3] Photo album on the counter, your cheeks were turnin\\' red You used to be a little kid with glasses in a twin-sized bed And your mother\\'s tellin\\' stories \\'bout you on the tee-ball team You taught me \\'bout your past, thinkin\\' your future was me And you were tossing me the car keys, \"Fuck the patriarchy\" Keychain on the ground, we were always skippin\\' town And I was thinkin\\' on the drive down, \"Any time now He\\'s gonna say it\\'s love,\" you never called it what it was \\'Til we were dead and gone and buried Check the pulse and come back swearin\\' it\\'s the same After three months in the grave And then you wondered where it went to as I reached for you But all I felt was shame and you held my lifeless frame'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "def get_country_music_lyrics_corpus():\n",
        "    \"\"\"Get the country music lyrics corpus.\"\"\"\n",
        "    raw_urls = [\n",
        "        \"https://raw.githubusercontent.com/life-efficient/Lyric-Generation/main/data/Country/Ashe-emotional-lyrics.txt\",\n",
        "        \"https://raw.githubusercontent.com/life-efficient/Lyric-Generation/main/data/Country/Creedence-clearwater-revival-have-you-ever-seen-the-rain-lyrics.txt\",\n",
        "        \"https://raw.githubusercontent.com/life-efficient/Lyric-Generation/main/data/Country/Hardy-and-lainey-wilson-wait-in-the-truck-lyrics.txt\",\n",
        "        \"https://raw.githubusercontent.com/life-efficient/Lyric-Generation/main/data/Country/Johnny-cash-folsom-prison-blues-lyrics.txt\",\n",
        "        \"https://raw.githubusercontent.com/life-efficient/Lyric-Generation/main/data/Country/Johnny-cash-ring-of-fire-lyrics.txt\",\n",
        "        \"https://raw.githubusercontent.com/life-efficient/Lyric-Generation/main/data/Country/Koe-wetzel-creeps-lyrics.txt\",\n",
        "        \"https://raw.githubusercontent.com/life-efficient/Lyric-Generation/main/data/Country/Rosa-linn-snap-lyrics.txt\",\n",
        "        \"https://raw.githubusercontent.com/life-efficient/Lyric-Generation/main/data/Country/Taylor-swift-all-too-well-10-minute-version-taylors-version-from-the-vault-lyrics.txt\"\n",
        "    ]\n",
        "    corpus = \"\"\n",
        "    for url in raw_urls:\n",
        "        response = requests.get(url)\n",
        "        lines = response.text.splitlines()\n",
        "        lines = [line for line in lines if line != '']\n",
        "        lyrics = \" \".join(lines)\n",
        "        corpus += lyrics\n",
        "    return corpus\n",
        "\n",
        "get_country_music_lyrics_corpus()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = get_country_music_lyrics_corpus()\n",
        "unique_chars = set(corpus)\n",
        "print(unique_chars)"
      ],
      "metadata": {
        "id": "Wq7zTeK8sU2z",
        "outputId": "29c15008-1835-41c7-9ff4-8aec85bd056e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'j', 'x', '1', 'd', 'q', '[', 'I', 'f', 'i', '2', 'u', 'F', 'a', 'L', 'T', '3', ' ', 'e', 'W', 'H', 'B', '“', 's', '/', '?', 'b', 'h', '\"', 'O', 'D', 'P', 't', \"'\", 'е', 'm', 'A', 'c', 'p', '!', 'N', 'V', 'z', 'k', 'S', '4', '0', ')', 'o', 'v', 'w', '9', 'r', 'J', '|', 'K', 'y', 'R', ',', '-', 'M', 'Y', 'U', 'G', ']', '(', '&', 'l', 'n', '5', ':', 'C', 'g'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjgv0k6vfBUB"
      },
      "source": [
        "### The Tokeniser\n",
        "\n",
        "![](https://github.com/AI-Core/Content-Public/blob/main/Content/units/Deep%20Learning%20for%20NLP/0.%20Intro%20to%20AI%20for%20Text%20Data/3.%20Building%20a%20simple%20character-level%20language%20model/images/Tokeniser.png?raw=1)\n",
        "\n",
        "The first thing we need to do is to create a tokeniser that can take in our raw text and split it into a sequence of tokens.\n",
        "\n",
        "In this simple example, we will create a character-level tokeniser:\n",
        "- The tokeniser should be able to encode any string into a sequence of character, then turn them into their integer index.\n",
        "- In most real applications, you'd use a word-level or subword-level tokeniser instead. \n",
        "- Here, we implement our own tokeniser for practice. In a real-world example, you can find pre-built tokenisers online, for example in [HuggingFace](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel.forward.example)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1JLBvjP-fBUB",
        "outputId": "f3bfb9e1-e7ce-4cd0-9610-3090ef5eace9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'w', 1: '9', 2: '[', 3: \"'\", 4: 'f', 5: '!', 6: 'q', 7: 'z', 8: 'h', 9: '5', 10: 'r', 11: 'd', 12: '“', 13: '(', 14: 'x', 15: '&', 16: '|', 17: 'k', 18: 'n', 19: 'g', 20: '1', 21: 'o', 22: 'i', 23: 'u', 24: '\"', 25: '2', 26: 'p', 27: 'v', 28: 'e', 29: 'b', 30: '0', 31: 'е', 32: '3', 33: '-', 34: ',', 35: 's', 36: ']', 37: '4', 38: 'l', 39: '/', 40: 'a', 41: 'y', 42: 'j', 43: 'm', 44: 't', 45: '?', 46: ')', 47: 'c', 48: ':', 49: ' '}\n",
            "Tokens: [8, 28, 38, 38, 21, 49, 0, 21, 10, 38, 11]\n",
            "Tokens: hello world\n"
          ]
        }
      ],
      "source": [
        "class Tokeniser:\n",
        "    def __init__(self, txt):\n",
        "        txt = self.preprocess(txt) # Preprocess the text\n",
        "        # TODO Create a set of unique characters in the input text\n",
        "        unique_chars = set(txt)\n",
        "        # TODO Get the vocabulary size\n",
        "        self.vocab_size = len(unique_chars)\n",
        "        # TODO Create a dictionary that maps character IDs to characters\n",
        "        self.id_to_token = dict(enumerate(unique_chars))\n",
        "        # TODO Create a reverse dictionary that maps characters to character IDs\n",
        "        self.token_to_id = {token: idx for idx, token in self.id_to_token.items()}\n",
        "\n",
        "    def preprocess(self, txt):\n",
        "        # TODO Convert the lyrics to lowercase\n",
        "        txt = txt.lower()\n",
        "        # other preprocessing steps can be added here\n",
        "        return txt\n",
        "\n",
        "    def encode(self, txt):\n",
        "        txt = self.preprocess(txt) # Preprocess\n",
        "        # TODO Encode the input string by mapping its characters to character IDs\n",
        "        token_ids = [self.token_to_id[token] for token in txt]\n",
        "        return token_ids\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        tokens = [self.id_to_token[token_id] for token_id in token_ids]# TODO Decode the input list of character IDs by mapping them to characters\n",
        "        return ''.join(tokens)\n",
        "\n",
        "corpus = get_country_music_lyrics_corpus()\n",
        "# TODO create a tokeniser object\n",
        "tokeniser = Tokeniser(corpus)\n",
        "print(tokeniser.id_to_token)\n",
        "# TODO encode a sentence\n",
        "tokens = tokeniser.encode('Hello World')\n",
        "print(\"Tokens:\", tokens)\n",
        "# TODO decode the tokens\n",
        "tokensid = tokeniser.decode(tokens)\n",
        "print(\"Tokens:\", tokensid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cno-XJYOfBUC"
      },
      "source": [
        "## Creating a simple character-level language modelling dataset\n",
        "\n",
        "A language modelling dataset consists of:\n",
        "- features: \n",
        "    - the sequential words/tokens in a body of text\n",
        "- targets: \n",
        "    - the next token for each position in time\n",
        "    - i.e. the features shifted one step forward in time\n",
        "\n",
        "Implementation details:\n",
        "- Like all PyTorch datasets, our dataset needs a `__len__` method. \n",
        "    - In this case, set the length of the dataset to be the number of chunks of text of the provided `chunk_size` that could fit in the dataset.\n",
        "- Define the `__getitem__` to get a random chunk of text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zWRQvh2ofBUC",
        "outputId": "77e01e35-248a-4fd7-ad4a-da0a34f85e91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset: 287\n",
            "First chunk of data:\n",
            "X: tensor([28, 34, 24, 49, 41, 21, 23, 49, 18, 28, 27, 28, 10, 49, 47, 40, 38, 38,\n",
            "        28, 11, 49, 22, 44, 49,  0,  8, 40, 44, 49, 22])\n",
            "Y: tensor(44)\n",
            "Sequence so far: e,\" you never called it what i\n",
            "Target next character: t\n",
            "X: tensor([28, 40, 18, 22, 18, 19, 49, 16, 49, 27, 28, 10, 22,  4, 22, 28, 11, 49,\n",
            "         2, 27, 28, 10, 35, 28, 49, 25, 36, 49, 41, 28])\n",
            "Y: tensor(35)\n",
            "Sequence so far: eaning | verified [verse 2] ye\n",
            "Target next character: s\n",
            "X: tensor([44, 22, 43, 28, 49,  3, 44, 22, 38, 49,  4, 21, 10, 28, 27, 28, 10, 34,\n",
            "        49, 21, 18, 49, 22, 44, 49, 19, 21, 28, 35, 49])\n",
            "Y: tensor(44)\n",
            "Sequence so far: time 'til forever, on it goes \n",
            "Target next character: t\n",
            "X: tensor([49, 21,  4, 49,  4, 22, 10, 28, 49, 22, 49,  0, 28, 18, 44, 49, 11, 21,\n",
            "         0, 18, 34, 49, 11, 21,  0, 18, 34, 49, 11, 21])\n",
            "Y: tensor(0)\n",
            "Sequence so far:  of fire i went down, down, do\n",
            "Target next character: w\n",
            "X: tensor([49,  0, 40, 35, 49, 47, 21, 38, 11, 49, 29, 23, 44, 49, 35, 21, 43, 28,\n",
            "        44,  8, 22, 18,  3, 49,  3, 29, 21, 23, 44, 49])\n",
            "Y: tensor(22)\n",
            "Sequence so far:  was cold but somethin' 'bout \n",
            "Target next character: i\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LyricDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokeniser, chunk_size=30):\n",
        "        \"\"\"\n",
        "        Initialize a LyricDataset object.\n",
        "        \n",
        "        Parameters:\n",
        "        chunk_size (int): The size of each chunk of data to be returned by the iterator.\n",
        "        \"\"\"\n",
        "        self.chunk_size = chunk_size  # The size of each chunk of data to be returned by the iterator\n",
        "        self.tokeniser = tokeniser\n",
        "\n",
        "        txt = get_country_music_lyrics_corpus()\n",
        "\n",
        "        # TODO Encode the text and store it in a tensor\n",
        "        self.X = torch.tensor(tokeniser.encode(txt))\n",
        "        # TODO Shift the encoded text by one character and store it in a tensor\n",
        "        #self.Y = np.roll(self.X,-1)\n",
        "        # TODO Store the size of the vocabulary (i.e. the number of unique characters in the text)\n",
        "        self.corpus_length = len(txt)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X) // self.chunk_size # TODO return the number of chunks in the dataset\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # TODO Select a random starting index for the chunk\n",
        "        start_idx = np.random.randint(0, self.corpus_length - self.chunk_size - 1)\n",
        "        # Select the chunk using a slice object\n",
        "        return self.X[start_idx: start_idx + self.chunk_size], self.X[start_idx+self.chunk_size] # TODO return the chunk of data and the target character\n",
        "\n",
        "\n",
        "# TODO create a dataset object\n",
        "dataset = LyricDataset(tokeniser)\n",
        "\n",
        "# print(\"Vocabulary size:\", dataset.vocab_size)\n",
        "print(\"Length of dataset:\", len(dataset))\n",
        "print(\"First chunk of data:\")\n",
        "for idx, (x, y) in enumerate(dataset):\n",
        "    print(\"X:\", x)\n",
        "    print(\"Y:\", y)\n",
        "\n",
        "    print(\"Sequence so far:\", tokeniser.decode(list(int(xx) for xx in x)))\n",
        "    print(\"Target next character:\", tokeniser.decode([int(y)]))\n",
        "    if idx > 3:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws54NkMjfBUD"
      },
      "source": [
        "Your labels should look the same as your features, just shifted by one position in time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbjdZXa5fBUD"
      },
      "source": [
        "Now let's make a dataloader to batch and shuffle the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aa5XGDUhfBUE",
        "outputId": "39bbe281-0c20-47a6-d13a-895ce40b199d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First batch of data:\n",
            "X: tensor([[ 8, 28, 41,  3, 10, 28, 49, 26, 10, 21, 29, 40, 29, 38, 41, 49, 11, 10,\n",
            "         22, 18, 17, 22, 18,  3, 49, 47, 21,  4,  4, 28],\n",
            "        [21, 10, 43, 49, 22, 49, 17, 18, 21,  0, 34, 49, 22, 44,  3, 35, 49, 29,\n",
            "         28, 28, 18, 49, 47, 21, 43, 22, 18,  3, 49,  4],\n",
            "        [41, 21, 23, 49, 28, 27, 28, 10, 49, 35, 28, 28, 18, 49, 44,  8, 28, 49,\n",
            "         10, 40, 22, 18, 45, 49, 47, 21, 43, 22, 18,  3],\n",
            "        [ 8, 28, 41, 49, 18, 28, 27, 28, 10, 49, 11, 21, 49, 44, 23, 10, 18, 35,\n",
            "         49, 21, 23, 44, 49, 26, 28, 21, 26, 38, 28, 49],\n",
            "        [18,  3, 44, 49, 11, 21, 49,  0,  8, 40, 44, 49,  8, 28, 49, 11, 22, 11,\n",
            "         49,  8, 28, 49,  0, 40, 35, 49,  8, 28, 38, 38],\n",
            "        [49,  8, 40, 27, 28, 49, 44, 21, 49, 35, 44, 40, 41, 49, 44,  8, 28, 49,\n",
            "         18, 22, 19,  8, 44, 34, 49, 21, 18, 38, 41, 49],\n",
            "        [49, 21, 18, 49, 44,  8, 28, 49, 19, 10, 21, 23, 18, 11, 34, 49,  0, 28,\n",
            "         49,  0, 28, 10, 28, 49, 40, 38,  0, 40, 41, 35],\n",
            "        [19, 49,  3, 10, 21, 23, 18, 11, 49, 44,  8, 28, 49, 29, 28, 18, 11, 49,\n",
            "         40, 18, 11, 49, 22, 49, 40, 22, 18,  3, 44, 49],\n",
            "        [38, 22, 18,  3, 49, 21, 18, 49, 11, 21,  0, 18, 49, 44, 21, 49, 35, 40,\n",
            "         18, 49, 40, 18, 44, 21, 18, 28, 49,  2, 27, 28],\n",
            "        [41, 21, 23, 49, 17, 28, 28, 26, 49, 41, 28, 40,  8, 34, 49, 44,  8, 40,\n",
            "         44, 49, 35,  8, 22, 44, 49, 19, 22, 27, 28, 35],\n",
            "        [17, 28, 41, 35, 34, 49, 24,  4, 23, 47, 17, 49, 44,  8, 28, 49, 26, 40,\n",
            "         44, 10, 22, 40, 10, 47,  8, 41, 24, 49, 17, 28],\n",
            "        [ 8, 28, 49, 10, 22, 18, 19, 49, 21,  4, 49,  4, 22, 10, 28,  2, 22, 18,\n",
            "         44, 10, 21, 36, 49, 13, 18, 40, 33, 18, 40, 33],\n",
            "        [18, 11, 49, 22,  3, 11, 49, 38, 28, 44, 49, 44,  8, 40, 44, 49, 38, 21,\n",
            "         18, 28, 35, 21, 43, 28, 49,  0,  8, 22, 35, 44],\n",
            "        [44, 49, 35,  8, 28, 49, 11, 22, 11, 18,  3, 44, 49,  8, 40, 27, 28, 49,\n",
            "         44, 21, 49, 22, 49, 17, 18, 28,  0, 49,  0,  8],\n",
            "        [21, 49, 19, 28, 44, 49, 28, 43, 21, 44, 22, 21, 18, 40, 38, 49,  2, 47,\n",
            "          8, 21, 10, 23, 35, 36, 49, 41, 21, 23,  3, 38],\n",
            "        [44, 49,  8, 22, 19,  8, 28, 10, 49, 40, 18, 11, 49, 22, 44, 49, 29, 23,\n",
            "         10, 18, 35, 34, 49, 29, 23, 10, 18, 35, 34, 49]])\n",
            "Y: tensor([28, 21, 49, 38, 29, 22, 49, 35, 10, 49, 41, 18, 38, 40, 38, 29])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# TODO create a dataloader object\n",
        "dataloader = DataLoader(dataset, batch_size = 16, shuffle = True)\n",
        "\n",
        "print(\"First batch of data:\")\n",
        "example_batch = next(iter(dataloader))\n",
        "print(\"X:\", example_batch[0])\n",
        "print(\"Y:\", example_batch[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr_ALkZJfBUE"
      },
      "source": [
        "## Defining the RNN model\n",
        "\n",
        "One of the simplest kinds of language models you can implement is using a many-to-one recurrent neural network that processes a sequence of many tokens to produce one classification - a classification of which word comes next.\n",
        "\n",
        "![](https://github.com/AI-Core/Content-Public/blob/main/Content/units/Deep%20Learning%20for%20NLP/0.%20Intro%20to%20AI%20for%20Text%20Data/3.%20Building%20a%20simple%20character-level%20language%20model/images/RNN%20Text%20Classifier.png?raw=1)\n",
        "\n",
        "Firstly, to initialise the model, we'll define the modules that will be needed to make the forward pass:\n",
        "- An embedding layer that takes in a sequence of token ids and turns them into a sequence of embeddings.\n",
        "    - See the docs [here](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html).\n",
        "    - When called on a sequence of length $T$, an embedding layer that produces $d$ dimensional embeddings for each token will output a matrix of size ($T$, $d$), which represents a $d$ dimensional token embedding for each of the $T$ timesteps.\n",
        "- An RNN layer\n",
        "    - Requires an embedding size $d$\n",
        "    - Requires a hidden size $h$\n",
        "    - Can be multi-layer\n",
        "- A classification head\n",
        "    - Will combine the final hidden state activations into logits for a classification\n",
        "        - We'll output the logits rather than the probabilities so that we can train the model using the `cross_entropy` loss function\n",
        "    - The classification should have the same dimensionality as the vocab size - a probability for each word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "U9Tv2e8yfBUE",
        "outputId": "8ae27692-174a-40a3-88a8-08c7f763e682",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden after initialisation tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
            "hidden shape torch.Size([1, 2, 32])\n"
          ]
        }
      ],
      "source": [
        "class RNN(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size=32 , hidden_size=32, n_layers=1):\n",
        "        # TODO initialise parent class\n",
        "        super().__init__()\n",
        "        # STORE HYPERPARAMETERS\n",
        "        self.vocab_size = vocab_size \n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # DEFINE MODEL MODULES\n",
        "        # TODO inintialise embedding layer\n",
        "        self.embedding = torch.nn.Embedding(self.vocab_size, embedding_size)\n",
        "        # TODO initialise RNN layer\n",
        "        self.rnn = torch.nn.RNN(embedding_size, hidden_size, n_layers, batch_first=True)\n",
        "        # TODO initialise classification head\n",
        "        self.classification_head = torch.nn.Linear(hidden_size, self.vocab_size)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        self.hidden = torch.zeros((self.n_layers, batch_size, self.hidden_size)) # we will do this in the next step\n",
        "    \n",
        "\n",
        "    def forward(self, x):\n",
        "        pass # we will do this in the next step\n",
        "\n",
        "\n",
        "rnn = RNN(tokeniser.vocab_size)\n",
        "rnn.init_hidden(batch_size=2)\n",
        "print('Hidden after initialisation', rnn.hidden)\n",
        "print('hidden shape', rnn.hidden.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk-bJTI0fBUF"
      },
      "source": [
        "Every computation performed by an RNN depends on having an initial hidden state. As per the equations, is needs to be combined with the input data at each timestep.\n",
        "\n",
        "> Typically, we initialise the hidden state of an RNN as a vector of zeros.\n",
        "\n",
        "Check out the [docs](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#rnn) to make sure you implement the correct shaped tensor.\n",
        "\n",
        "So let's define a method that does that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dDLY64QfBUF"
      },
      "outputs": [],
      "source": [
        "class RNN(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size=32, hidden_size=32, n_layers=1):\n",
        "        # TODO initialise parent class\n",
        "\n",
        "        # STORE HYPERPARAMETERS\n",
        "        self.vocab_size = vocab_size \n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # DEFINE MODEL MODULES\n",
        "        # TODO inintialise embedding layer\n",
        "        # TODO initialise RNN layer\n",
        "        # TODO initialise classification head\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # TODO initialise hidden state\n",
        "\n",
        "    def forward(self, x):\n",
        "        pass # we will do this in the next step\n",
        "\n",
        "\n",
        "rnn = RNN(tokeniser.vocab_size)\n",
        "# print(\"Hidden before initialisation:\", rnn.hidden)\n",
        "rnn.init_hidden(batch_size=2)\n",
        "print(\"Hidden after initialisation:\", rnn.hidden)\n",
        "print(\"Hidden shape:\", rnn.hidden.shape) # (L, B, H)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn5xFVE5fBUF"
      },
      "source": [
        "Now let's define the forward pass.\n",
        "\n",
        "Torch's recurrent layers are a little different to other layers in a few ways:\n",
        "1. The first dimension is not the batch dimension by default! Instead, it's the time dimension, followed by the batch dimension.\n",
        "1. They take in more than one argument:\n",
        "    - The input data, as usual\n",
        "    - The current hidden state\n",
        "1. They return more than one thing:\n",
        "    - The final hidden values of every layer\n",
        "    - The output from each timestep\n",
        "\n",
        "![](https://github.com/AI-Core/Content-Public/blob/main/Content/units/Deep%20Learning%20for%20NLP/0.%20Intro%20to%20AI%20for%20Text%20Data/3.%20Building%20a%20simple%20character-level%20language%20model/images/PyTorch%20RNN%20Outputs.png?raw=1)\n",
        "\n",
        "The output from each timestep is the activations of the final recurrent layer for every timestep.\n",
        "\n",
        "In our case, we won't need to use the hidden states output from the RNN layer.\n",
        "\n",
        "These behaviours might seem unusual, but as you get more familiar with using recurrent networks, you'll realise how they can be useful and make RNNs very flexible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NroYovlffBUG",
        "outputId": "9635a3ba-93f6-4d45-a66b-2b0a5cc4c838",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size: 16\n",
            "Sequence length: 30\n",
            "Vocabulary size: 50\n",
            "torch.Size([16, 30, 50])\n"
          ]
        }
      ],
      "source": [
        "class RNN(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size=32, hidden_size=32, n_layers=1):\n",
        "        # TODO initialise parent class\n",
        "        super().__init__()\n",
        "        # STORE HYPERPARAMETERS\n",
        "        self.vocab_size = vocab_size \n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # DEFINE MODEL MODULES\n",
        "        # TODO inintialise embedding layer\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
        "        # TODO initialise RNN layer\n",
        "        self.rnn = torch.nn.RNN(embedding_size, hidden_size, n_layers, batch_first = True)\n",
        "        # TODO initialise classification head\n",
        "        self.classification_head = torch.nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        self.hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)# TODO initialise hidden state\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.init_hidden(X.shape[0])\n",
        "        embedding = self.embedding(X)\n",
        "        outputs, final_hidden = self.rnn(embedding, self.hidden)\n",
        "        predictions = self.classification_head(outputs)\n",
        "        return predictions\n",
        "\n",
        "\n",
        "features, labels = example_batch\n",
        "print(\"Batch size:\", features.shape[0])\n",
        "print(\"Sequence length:\", features.shape[1])\n",
        "print(\"Vocabulary size:\", tokeniser.vocab_size)\n",
        "rnn = RNN(tokeniser.vocab_size)\n",
        "prediction = rnn(features)\n",
        "print(prediction.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjaqrOaEfBUG"
      },
      "source": [
        "## Generating new text\n",
        "\n",
        "Now we need to implement a method of our model that takes what it knows and uses it to generate new text.\n",
        "\n",
        "Initially, our generated text will be awful, because we haven't trained the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "rzE_bO2OfBUG",
        "outputId": "e317357d-ecd8-4390-f1c1-eec1e0327ef1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-ccdd2f185d74>\"\u001b[0;36m, line \u001b[0;32m41\u001b[0m\n\u001b[0;31m    for idx in np.arange(100):  # generate 100 character sequence\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "class RNN(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size=32, hidden_size=32, n_layers=1):\n",
        "        # TODO initialise parent class\n",
        "        super().__init__()\n",
        "        # STORE HYPERPARAMETERS\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # DEFINE MODEL MODULES\n",
        "        # TODO inintialise embedding layer\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
        "        # TODO initialise RNN layer\n",
        "        self.rnn = torch.nn.RNN(\n",
        "            embedding_size, hidden_size, n_layers, batch_first=True)\n",
        "        self.classification_head = torch.nn.Linear(hidden_size, vocab_size)\n",
        "            # TODO initialise classification head\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # TODO initialise hidden state\n",
        "        self.hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.init_hidden(X.shape[0])\n",
        "        embedding = self.embedding(X)\n",
        "        outputs, final_hidden = self.rnn(embedding, self.hidden)\n",
        "        outputs = outputs[:, -1] # get last output\n",
        "        predictions = self.classification_head(outputs)\n",
        "        return predictions\n",
        "\n",
        "    def generate(self):\n",
        "        self.init_hidden(batch_size=1)\n",
        "        initial_token_id = random.randint(0, 49-1)\n",
        "        generated_token_ids = [initial_token_id]\n",
        "        initial_token_batch = torch.tensor(initial_token_id).unsqueeze(\n",
        "            # TODO SOS token\n",
        "        embedding = self.embedding(initial_token_batch)\n",
        "        for idx in range(100):  # generate 100 character sequence\n",
        "            outputs, self.hidden = self.rnn(embedding, self.hidden)\n",
        "            predictions = self.classification_head(outputs)\n",
        "            # outputs has shape BxLxN=1x1xN\n",
        "            predictions = predictions.squeeze()  # remove 1-dims\n",
        "            chosen_token_id = torch.argmax(predictions)\n",
        "            generated_token_ids.append(int(chosen_token_id))\n",
        "            embedding = self.embedding(\n",
        "                chosen_token_id).unsqueeze(0).unsqueeze(0)\n",
        "        return generated_token_ids\n",
        "\n",
        "\n",
        "rnn = RNN(tokeniser.vocab_size)\n",
        "myrnn = RNN(tokeniser.vocab_size, 32, 32, 1)\n",
        "\n",
        "generated_tokens = rnn.generate()\n",
        "print(\"Generated text:\", tokeniser.decode(generated_tokens))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fclE0DIKfBUG"
      },
      "source": [
        "## Creating the training loop\n",
        "\n",
        "Now we have the model and the dataset, we need to pass the model through the dataset repeatedly and iteratively optimise the model parameters using gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rEAPNT4afBUH",
        "outputId": "cca6b0f9-110b-432c-e71a-e8ced40deb70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4c2c8bd655d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# LOAD DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_country_music_lyrics_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mtokeniser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokeniser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLyricDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokeniser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_country_music_lyrics_corpus' is not defined"
          ]
        }
      ],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def train(model, dataset, tokeniser, epochs=1):\n",
        "    writer = SummaryWriter()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)  # choose optimiser\n",
        "    n_steps = 0\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0  # stored the loss per epoch\n",
        "        for X, y in dataloader:\n",
        "            \n",
        "            predictions = model(X)\n",
        "            # seq_targets = seq_targets.unsqueeze(0)\n",
        "            # predictions = predictions.view(-1, predictions.shape[-1])\n",
        "            # seq_targets = seq_targets.view(-1)  # BxT targets all in a line\n",
        "            # print(tokeniser.decode([int(x) for x in X[0, -20:]]))\n",
        "\n",
        "            # print(tokeniser.decode([int(torch.argmax(y[0]))]))\n",
        "            loss = F.cross_entropy(predictions, y)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # OPTIMISE\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # LOGGING\n",
        "            writer.add_scalar(\"Loss/Train\", loss.item(), n_steps)\n",
        "            n_steps += 1\n",
        "\n",
        "        epoch_loss /= len(dataset)  # avg loss per epoch\n",
        "\n",
        "        print('Epoch ', epoch, ' Avg loss/chunk: ', epoch_loss)\n",
        "        generated_token_ids = model.generate()\n",
        "        writer.add_text(\"Generated Text\", tokeniser.decode(\n",
        "            generated_token_ids)[:300], epoch)\n",
        "            # TODO stop on EOS token\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # HYPER-PARAMS\n",
        "    lr = 0.05\n",
        "    epochs = 5000\n",
        "    chunk_size = 30  # the length of the sequences which we will optimize over\n",
        "    batch_size = 32\n",
        "\n",
        "    # MODEL ARCHITECTURE\n",
        "    embedding_size = 64\n",
        "    hidden_size = 64\n",
        "    n_layers = 2\n",
        "\n",
        "    # LOAD DATA\n",
        "    corpus = get_country_music_lyrics_corpus()\n",
        "    tokeniser = Tokeniser(corpus)\n",
        "    dataset = LyricDataset(tokeniser, chunk_size=chunk_size)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    n_tokens = len(dataset.tokeniser.id_to_token)\n",
        "    # instantiate our model from the class defined earlier\n",
        "    myrnn = RNN(n_tokens, embedding_size, hidden_size, n_layers)\n",
        "    train(myrnn, dataset, tokeniser, epochs)\n",
        "    # myrnn = RNN(n_tokens, hidden_size, n_layers)\n",
        "    # train(myrnn, dataset, epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2_PTnWGfBUH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "content-projects_new",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "4b961f8166aad6ccb4cf65d0f9c742ef9c6c23ffe83ad932438cd83ed96aebaf"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}