{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy59S-W_Ku-h"
      },
      "source": [
        "# Understanding Neural Networks\n",
        "\n",
        "Remember that machine learning models are mathematical functions that perform some input-output transformation.\n",
        "The goal of machine learning models is to learn the function they should represent.\n",
        "\n",
        "More complex problems have more complex input-output relationships, and that means they need to be represented by more complicated models.\n",
        "Models with the ability to represent more complicated functions are said to have a higher _capacity_.\n",
        "\n",
        "> Neural networks are a type of model that can learn to represent very complicated input-output relationships\n",
        "\n",
        "If you know how a logistic regression model is built and trained, then you already understand everything you need to know how neural networks work.\n",
        "\n",
        "Logistic regression works by:\n",
        "- Applying a linear transformation followed by a non-linear activation function (the softmax)\n",
        "- Using the chain rule of differentiation to determine how changes to model parameters change the output, that is, to determine the gradient\n",
        "\n",
        "> The idea behind neural networks is that by applying sequential transformations, we can represent a more complicated function. \n",
        "\n",
        "![](https://github.com/AI-Core/Content-Public/blob/main/Content/units/Towards%20ChatGPT/3.%20Neural%20Networks/0.%20Understanding%20Neural%20Networks/images/Linear%20Regression%20vs%20NN%20Graphical%20Model.png?raw=1)\n",
        "\n",
        "We call these intermediate layers of transformation, between the input and the output, _hidden layers_.\n",
        "\n",
        "The typical diagram of a neural network shows the individual nodes in each hidden layer.\n",
        "\n",
        "> In a neural network, each sequential layer combines features to build more complex \n",
        "\n",
        "![](https://github.com/AI-Core/Content-Public/blob/main/Content/units/Towards%20ChatGPT/3.%20Neural%20Networks/0.%20Understanding%20Neural%20Networks/images/Linear%20Regression%20vs%20NN.png?raw=1)\n",
        "\n",
        "And these neural networks have parameters between layers similar to those found in linear models:\n",
        "\n",
        "![](https://github.com/AI-Core/Content-Public/blob/main/Content/units/Towards%20ChatGPT/3.%20Neural%20Networks/0.%20Understanding%20Neural%20Networks/images/NN%20Showing%20Weights.png?raw=1)\n",
        "\n",
        "We will get onto how each value in each layer is calculated shortly.\n",
        "\n",
        "The width and depth of a neural network, make up the part of the _neural network architecture_. Neural networks can have any number of hidden layers, with any width, and any number of inputs and outputs.\n",
        "\n",
        "![](https://github.com/AI-Core/Content-Public/blob/main/Content/units/Towards%20ChatGPT/3.%20Neural%20Networks/0.%20Understanding%20Neural%20Networks/images/NN%20with%20Many%20Layers.png?raw=1)\n",
        "\n",
        "That means that you can train neural networks to represent functions with any different kind of sized inputs and outputs.\n",
        "\n",
        "![](https://github.com/AI-Core/Content-Public/blob/main/Content/units/Towards%20ChatGPT/3.%20Neural%20Networks/0.%20Understanding%20Neural%20Networks/images/Different%20Vanilla%20NN%20Architectures.png?raw=1)\n",
        "\n",
        "There are a few more details to go into (particularly the activation function used below), but firstly, take a look at how easily building a neural network can be accomplished in PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0KQfyuuKKu-l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class NeuralNetwork(torch.nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs):\n",
        "        super().__init__()\n",
        "        self.activation_function = torch.nn.functional.relu\n",
        "        self.hidden_layer_1 = torch.nn.Linear(num_inputs, 256)\n",
        "        self.hidden_layer_2 = torch.nn.Linear(256, 128)\n",
        "        self.hidden_layer_3 = torch.nn.Linear(128, 128)\n",
        "        # you could have many more hidden layers here\n",
        "        self.output_layer = torch.nn.Linear(128, num_outputs)\n",
        "\n",
        "    def forward(self, X):\n",
        "        hidden_layer_1_activations = self.activation_function(self.hidden_layer_1(X))\n",
        "        hidden_layer_2_activations = self.activation_function(self.hidden_layer_1(hidden_layer_1_activations))\n",
        "        hidden_layer_3_activations = self.activation_function(self.hidden_layer_2(hidden_layer_2_activations))\n",
        "        output = self.output_layer(hidden_layer_3_activations)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6SlEQRAKu-n"
      },
      "source": [
        "Even better, would be to use `torch.nn.Sequential` to avoid having to come up with clear names for each layer. \n",
        "It takes in a number of different torch modules (transformations) and returns a callable object.\n",
        "When called, each of these transformations is applied sequentially to the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_TKTqlJFKu-o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class NeuralNetwork(torch.nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(num_inputs, 256),\n",
        "            torch.nn.ReLU(), # relu activation function\n",
        "            torch.nn.Linear(256, 128),\n",
        "            torch.nn.ReLU(), # relu activation function\n",
        "            torch.nn.Linear(128, 128),\n",
        "            torch.nn.ReLU(), # relu activation function\n",
        "            # you could have many more hidden layers here\n",
        "            torch.nn.Linear(128, num_outputs)\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.layers(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6D75FLOKu-o"
      },
      "source": [
        "Note that this model would have been tough to write out the calculations for the gradient of the output with respect to the model parameters, but PyTorch takes care of all of this for us in the background."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6JKOdlCKu-o"
      },
      "source": [
        "## What are those activation functions?\n",
        "\n",
        "> An _activation function_ is any non-linear function applied to the linear output of a hidden layer\n",
        "\n",
        "Here are some of the most common activation functions:\n",
        "\n",
        "![](https://github.com/AI-Core/Content-Public/blob/main/Content/units/Towards%20ChatGPT/3.%20Neural%20Networks/0.%20Understanding%20Neural%20Networks/images/Common%20Activation%20Functions.png?raw=1)\n",
        "\n",
        "> It is important that the activation function is differentiable, like every module in our neural networks, because the model will be trained using gradient descent\n",
        "\n",
        "With the activation functions, the full forward pass (for a single hidden layer neural network) computes the following:\n",
        "\n",
        "![](https://github.com/AI-Core/Content-Public/blob/main/Content/units/Towards%20ChatGPT/3.%20Neural%20Networks/0.%20Understanding%20Neural%20Networks/images/NN%20Full%20Forward%20Pass%20Equation.png?raw=1)\n",
        "\n",
        "### Why do we need activation functions?\n",
        "\n",
        "> If you leave out the activation function, the forward pass of a neural network is equivalent to a single linear transformation. You added all those parameters, and didn't increase the capacity of the model!\n",
        "\n",
        "Below is a simple proof. \n",
        "\n",
        "_Note that biases are removed for simplification (a weighted sum plus bias can be rewritten as a single matrix multiplication, so the example below still holds)_\n",
        "\n",
        "![](https://github.com/AI-Core/Content-Public/blob/main/Content/units/Towards%20ChatGPT/3.%20Neural%20Networks/0.%20Understanding%20Neural%20Networks/images/Why%20you%20Need%20Activation%20Functions.png?raw=1)\n",
        "\n",
        "In general, stick with ReLU as your activation function.\n",
        "\n",
        "> There are so many activation functions that work comparably well, that research on new activation functions has become uninteresting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VzyP8tnKu-p"
      },
      "source": [
        "## Neural networks build hierarchical representations of the data\n",
        "\n",
        "> It's easier to learn input-output transformations step by step (layer by layer), rather than all at once.\n",
        "\n",
        "For example:\n",
        "- it's easy to learn a function that detects a line at a certain angle\n",
        "- it is much harder to learn the function which detects what the make and model of a car in an image, directly from the raw pixels, all at once\n",
        "\n",
        "Neural networks learn simple functions layer by layer. Their output is then the input to the next layer. The inputs to each successive layer are more meaningful and high level, and they can be combined to represent even more complex features.\n",
        "\n",
        "![](https://github.com/AI-Core/Content-Public/blob/main/Content/units/Towards%20ChatGPT/3.%20Neural%20Networks/0.%20Understanding%20Neural%20Networks/images/nnfeaturevis.png?raw=1)\n",
        "\n",
        "- The first hidden layer computes features that are combinations of the inputs\n",
        "    - e.g. combine raw pixel values to create edges\n",
        "- The next hidden layer computes more complex features, that are combinations of the previous layer\n",
        "    - e.g. combine edges to create shapes\n",
        "- Sequential layers continue to compute more complex features, that are combinations of the previous layer\n",
        "    - e.g. combine shapes to represent objects\n",
        "- And so on\n",
        "    - e.g. deeper layers are building combining objects to build abstract, meaningful representations\n",
        "- The output layer can easily combine the meaningful representations to make predictions\n",
        "    - The high level features extracted by the deeper layers are the kind of things that can eventually be used to make direct predictions from.\n",
        "        - e.g. if you know that an image contains a wheel, you can be confident in a prediction of the image containing a car\n",
        "        - but you can't tell this just by looking at the value of the pixels\n",
        "\n",
        "> Neural networks turn low-level (raw, simple) features into high-level (complex, abstract, meaningful) features that predictions can be made from before using them to make a prediction\n",
        "\n",
        "Simple models try to make predictions directly from a combination of the raw inputs, in a single transformation. \n",
        "Neural networks, on the other hand, apply sequential intermediate transformations, where each layer builds up on the output of the layer before.\n",
        "\n",
        "> Neural networks are different to many other types of models because they build hierarchical representations of data\n",
        "\n",
        "## What do the values of the activations represent?\n",
        "\n",
        "The function that a neural network represents, is based on the parameters in each layer, which are initialised randomly. That means, to start with, the neural network is a random function. We'll train it shortly using gradient descent.\n",
        "\n",
        "> The values of activations in layers represent the presence of features in the input, and deeper layers represent more complex features\n",
        "\n",
        "## Neural networks learn their own representations for the data\n",
        "\n",
        "Whereas simple models are forced to make a prediction based on the way that data is provided to them, neural networks learn how to represent the data through the hidden layers, so that the final layer receives useful inputs.\n",
        "\n",
        "> Neural networks learn to extract useful features from the input, by learning for themselves what it would be useful for each activation to represent "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bByrXX6NKu-p"
      },
      "source": [
        "## Notice that neural networks can have lots of parameters\n",
        "\n",
        "- Vector outputs of hidden layers = weight matrices instead of just weight vectors\n",
        "- Wider hidden layers = larger weight matrices\n",
        "- More hidden layers = more weight matrices\n",
        "\n",
        "These additional parameters are part of what gives neural networks a much greater representational capacity than simple (e.g. linear) models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "X_sB-sNPKu-q",
        "outputId": "1f662592-5b57-4ec8-e019-4e0cf61e0dd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# model parameters: 50561\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    n_params = 0\n",
        "    for param in model.parameters():\n",
        "        n_params += param.numel()\n",
        "    print(\"# model parameters:\", n_params)\n",
        "\n",
        "model = NeuralNetwork(3, 1)\n",
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dD3m6JtKu-q"
      },
      "source": [
        "## Neural Network Architectures\n",
        "\n",
        "These neural networks that simply process vectors through linear layers with activation functions are often referred to as:\n",
        "- \"Vanilla\" neural networks\n",
        "- Feedforward neural networks\n",
        "- Multi-layer perceptrons (MLP)\n",
        "\n",
        "There are many more advanced neural network architectures that specialise in different ways. Common ones you may have heard of include:\n",
        "- Convolutional Neural Networks\n",
        "- Recurrent Neural Networks\n",
        "- Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HmT0PoaKu-r"
      },
      "source": [
        "## Training neural networks\n",
        "\n",
        "Neural networks can be trained from end-to-end using gradient descent and the chain rule of differentiation.\n",
        "\n",
        "A key thing to notice is that for many of the gradient calculations, some terms reappear for different parameters.\n",
        "\n",
        "![](https://github.com/AI-Core/Content-Public/blob/main/Content/units/Towards%20ChatGPT/3.%20Neural%20Networks/0.%20Understanding%20Neural%20Networks/images/Backpropagation.png?raw=1)\n",
        "\n",
        "> Backpropagation specifically refers to the algorithm that caches these terms that repeat, so that they don't have to be repeatedly computed.\n",
        "\n",
        "PyTorch handles this all efficiently under the hood when you call `.backward()` on any variable computed from the model's output (probably the loss) to populate the parameter gradients.\n",
        "\n",
        "Now, you understand everything required to train a neural network in PyTorch.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RfSCkYpKu-r"
      },
      "source": [
        "This cell just creates a dummy dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZbaiLn4xKu-r"
      },
      "outputs": [],
      "source": [
        "def create_dummy_dataset(num_examples, num_features):\n",
        "    X = torch.randn((num_examples, num_features))\n",
        "    y = torch.randn((num_examples, 1)) # 1 label each\n",
        "    return X, y\n",
        "\n",
        "def create_dummy_dataloader(X, y, batch_size=4):\n",
        "    def create_batches(data):\n",
        "        return [\n",
        "            data[idx*batch_size: (idx+1) * batch_size] if (idx + 1) * batch_size < len(data)\n",
        "            else data[idx*batch_size:]\n",
        "            for idx in range(len(data) // batch_size)\n",
        "        ]\n",
        "    batched_X = create_batches(X)\n",
        "    batched_y = create_batches(y)\n",
        "    return list(zip(batched_X, batched_y))\n",
        "    \n",
        "X, y = create_dummy_dataset(10, 4)\n",
        "dataloader = create_dummy_dataloader(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5SEUYWYKu-s"
      },
      "source": [
        "Before we train the model, note that the dataset is random and hence the loss does not decay as it would with a real dataset.\n",
        "\n",
        "Overall, the training loop is the same as for a simple linear model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UkQ6aU5mKu-s",
        "outputId": "2bc88f86-4b07-410e-b8b5-7aa4c4e56dd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.9997310638427734\n",
            "Loss: 0.5485384464263916\n",
            "Loss: 0.18646766245365143\n",
            "Loss: 1.1001139879226685\n",
            "Loss: 0.526660680770874\n",
            "Loss: 0.563155472278595\n",
            "Loss: 0.7949528098106384\n",
            "Loss: 1.0755865573883057\n",
            "Loss: 1.6562820672988892\n",
            "Loss: 0.43399578332901\n",
            "Loss: 0.937506914138794\n",
            "Loss: 0.6343901753425598\n",
            "Loss: 0.42790907621383667\n",
            "Loss: 0.5763123035430908\n",
            "Loss: 0.6920958161354065\n",
            "Loss: 0.1597772091627121\n",
            "Loss: 1.0502029657363892\n",
            "Loss: 2.105548620223999\n",
            "Loss: 0.7269710898399353\n",
            "Loss: 0.9596070051193237\n",
            "Loss: 0.4411381781101227\n",
            "Loss: 1.7321791648864746\n",
            "Loss: 0.09839367121458054\n",
            "Loss: 1.516767978668213\n",
            "Loss: 1.345202088356018\n",
            "Loss: 1.0061438083648682\n",
            "Loss: 0.5942977070808411\n",
            "Loss: 0.1617986261844635\n",
            "Loss: 1.0496201515197754\n",
            "Loss: 0.4979972243309021\n",
            "Loss: 0.5742862224578857\n",
            "Loss: 0.7940011024475098\n",
            "Loss: 1.1167980432510376\n",
            "Loss: 1.6668615341186523\n",
            "Loss: 0.48737218976020813\n",
            "Loss: 0.898861289024353\n",
            "Loss: 0.6381609439849854\n",
            "Loss: 0.3870609998703003\n",
            "Loss: 0.5533945560455322\n",
            "Loss: 0.6283867359161377\n",
            "Loss: 0.16458760201931\n",
            "Loss: 1.0761123895645142\n",
            "Loss: 2.056196689605713\n",
            "Loss: 0.7457966804504395\n",
            "Loss: 0.9556305408477783\n",
            "Loss: 0.42307084798812866\n",
            "Loss: 1.7223670482635498\n",
            "Loss: 0.0970458984375\n",
            "Loss: 1.4351696968078613\n",
            "Loss: 1.3223717212677002\n",
            "Loss: 1.0165514945983887\n",
            "Loss: 0.6070705652236938\n",
            "Loss: 0.15645729005336761\n",
            "Loss: 1.0233943462371826\n",
            "Loss: 0.4987723231315613\n",
            "Loss: 0.5546467304229736\n",
            "Loss: 0.7940397262573242\n",
            "Loss: 1.1102575063705444\n",
            "Loss: 1.6561208963394165\n",
            "Loss: 0.5059562921524048\n",
            "Loss: 0.8836302757263184\n",
            "Loss: 0.6381663084030151\n",
            "Loss: 0.37489813566207886\n",
            "Loss: 0.5499771237373352\n",
            "Loss: 0.5939633846282959\n",
            "Loss: 0.16825571656227112\n",
            "Loss: 1.0761960744857788\n",
            "Loss: 2.0414748191833496\n",
            "Loss: 0.7556828260421753\n",
            "Loss: 0.9494754672050476\n",
            "Loss: 0.4173446297645569\n",
            "Loss: 1.7098543643951416\n",
            "Loss: 0.09991737455129623\n",
            "Loss: 1.3813613653182983\n",
            "Loss: 1.301924467086792\n",
            "Loss: 1.0266973972320557\n",
            "Loss: 0.6141568422317505\n",
            "Loss: 0.1547408103942871\n",
            "Loss: 1.0053120851516724\n",
            "Loss: 0.5046572685241699\n",
            "Loss: 0.5315399169921875\n",
            "Loss: 0.794398307800293\n",
            "Loss: 1.098175048828125\n",
            "Loss: 1.6377016305923462\n",
            "Loss: 0.5167937278747559\n",
            "Loss: 0.8748948574066162\n",
            "Loss: 0.6377617716789246\n",
            "Loss: 0.3698574900627136\n",
            "Loss: 0.5492620468139648\n",
            "Loss: 0.5680732131004333\n",
            "Loss: 0.1720677614212036\n",
            "Loss: 1.0718791484832764\n",
            "Loss: 2.030282497406006\n",
            "Loss: 0.7641832828521729\n",
            "Loss: 0.9442580938339233\n",
            "Loss: 0.4139319360256195\n",
            "Loss: 1.6986236572265625\n",
            "Loss: 0.10313904285430908\n",
            "Loss: 1.3396583795547485\n",
            "Loss: 1.2848589420318604\n",
            "Loss: 1.0337766408920288\n",
            "Loss: 0.6213241219520569\n",
            "Loss: 0.15293091535568237\n",
            "Loss: 0.9903334975242615\n",
            "Loss: 0.5096244215965271\n",
            "Loss: 0.5116580128669739\n",
            "Loss: 0.7951348423957825\n",
            "Loss: 1.0873219966888428\n",
            "Loss: 1.6209206581115723\n",
            "Loss: 0.5254155993461609\n",
            "Loss: 0.8675336837768555\n",
            "Loss: 0.63950115442276\n",
            "Loss: 0.36595338582992554\n",
            "Loss: 0.5490725636482239\n",
            "Loss: 0.5433920621871948\n",
            "Loss: 0.17509260773658752\n",
            "Loss: 1.0688897371292114\n",
            "Loss: 2.017972707748413\n",
            "Loss: 0.7722184658050537\n",
            "Loss: 0.9402918219566345\n",
            "Loss: 0.41016221046447754\n",
            "Loss: 1.6877987384796143\n",
            "Loss: 0.10629327595233917\n",
            "Loss: 1.3040534257888794\n",
            "Loss: 1.2679808139801025\n",
            "Loss: 1.0421934127807617\n",
            "Loss: 0.6293867230415344\n",
            "Loss: 0.15058746933937073\n",
            "Loss: 0.9749659895896912\n",
            "Loss: 0.5122212171554565\n",
            "Loss: 0.4957118630409241\n",
            "Loss: 0.7956700325012207\n",
            "Loss: 1.0790793895721436\n",
            "Loss: 1.6059188842773438\n",
            "Loss: 0.5343645215034485\n",
            "Loss: 0.8595767021179199\n",
            "Loss: 0.6404819488525391\n",
            "Loss: 0.3604719638824463\n",
            "Loss: 0.5491182804107666\n",
            "Loss: 0.5190997123718262\n",
            "Loss: 0.17664816975593567\n",
            "Loss: 1.0645766258239746\n",
            "Loss: 2.005131244659424\n",
            "Loss: 0.780913233757019\n",
            "Loss: 0.9359206557273865\n",
            "Loss: 0.4074556529521942\n",
            "Loss: 1.6762218475341797\n",
            "Loss: 0.11025629937648773\n",
            "Loss: 1.268207311630249\n",
            "Loss: 1.2555091381072998\n",
            "Loss: 1.0460205078125\n",
            "Loss: 0.6328243017196655\n",
            "Loss: 0.14965695142745972\n",
            "Loss: 0.9611542224884033\n",
            "Loss: 0.5163353681564331\n",
            "Loss: 0.47792214155197144\n",
            "Loss: 0.7967039942741394\n",
            "Loss: 1.0694667100906372\n",
            "Loss: 1.5870925188064575\n",
            "Loss: 0.5424792170524597\n",
            "Loss: 0.853853702545166\n",
            "Loss: 0.6416310667991638\n",
            "Loss: 0.35868534445762634\n",
            "Loss: 0.5486548542976379\n",
            "Loss: 0.4954701364040375\n",
            "Loss: 0.1788480132818222\n",
            "Loss: 1.0625232458114624\n",
            "Loss: 1.99165940284729\n",
            "Loss: 0.7884124517440796\n",
            "Loss: 0.9304013848304749\n",
            "Loss: 0.4037376344203949\n",
            "Loss: 1.664703130722046\n",
            "Loss: 0.11421890556812286\n",
            "Loss: 1.2292300462722778\n",
            "Loss: 1.240222692489624\n",
            "Loss: 1.0519520044326782\n",
            "Loss: 0.6411457061767578\n",
            "Loss: 0.14758610725402832\n",
            "Loss: 0.945897102355957\n",
            "Loss: 0.5171148180961609\n",
            "Loss: 0.4618053436279297\n",
            "Loss: 0.7981844544410706\n",
            "Loss: 1.0629807710647583\n",
            "Loss: 1.5680410861968994\n",
            "Loss: 0.5510446429252625\n",
            "Loss: 0.849285900592804\n",
            "Loss: 0.6436342000961304\n",
            "Loss: 0.35403013229370117\n",
            "Loss: 0.5485209822654724\n",
            "Loss: 0.4719417095184326\n",
            "Loss: 0.18012100458145142\n",
            "Loss: 1.0608446598052979\n",
            "Loss: 1.9727132320404053\n",
            "Loss: 0.797190248966217\n",
            "Loss: 0.9264240264892578\n",
            "Loss: 0.398921400308609\n",
            "Loss: 1.6537261009216309\n",
            "Loss: 0.1191493421792984\n",
            "Loss: 1.1909626722335815\n",
            "Loss: 1.2278555631637573\n",
            "Loss: 1.0548686981201172\n",
            "Loss: 0.6476152539253235\n",
            "Loss: 0.1463060975074768\n",
            "Loss: 0.9364959001541138\n",
            "Loss: 0.5170469880104065\n",
            "Loss: 0.4448907673358917\n",
            "Loss: 0.8010188341140747\n",
            "Loss: 1.0576660633087158\n",
            "Loss: 1.5519664287567139\n",
            "Loss: 0.5596295595169067\n",
            "Loss: 0.844577968120575\n",
            "Loss: 0.6439797878265381\n",
            "Loss: 0.34824562072753906\n",
            "Loss: 0.5495194792747498\n",
            "Loss: 0.44864416122436523\n",
            "Loss: 0.18100525438785553\n",
            "Loss: 1.0582858324050903\n",
            "Loss: 1.953768253326416\n",
            "Loss: 0.8090554475784302\n",
            "Loss: 0.9240845441818237\n",
            "Loss: 0.3943471908569336\n",
            "Loss: 1.6422489881515503\n",
            "Loss: 0.12458740919828415\n",
            "Loss: 1.1468802690505981\n",
            "Loss: 1.2148442268371582\n",
            "Loss: 1.0589821338653564\n",
            "Loss: 0.652969479560852\n",
            "Loss: 0.14573727548122406\n",
            "Loss: 0.9246840476989746\n",
            "Loss: 0.5176188349723816\n",
            "Loss: 0.42884859442710876\n",
            "Loss: 0.8017510175704956\n",
            "Loss: 1.0519827604293823\n",
            "Loss: 1.530358076095581\n",
            "Loss: 0.5681418776512146\n",
            "Loss: 0.840470552444458\n",
            "Loss: 0.6459107398986816\n",
            "Loss: 0.3457072377204895\n",
            "Loss: 0.550279438495636\n",
            "Loss: 0.4273858070373535\n",
            "Loss: 0.18340271711349487\n",
            "Loss: 1.0566617250442505\n",
            "Loss: 1.936089038848877\n",
            "Loss: 0.8137965202331543\n",
            "Loss: 0.920066773891449\n",
            "Loss: 0.39018514752388\n",
            "Loss: 1.63214111328125\n",
            "Loss: 0.1302063763141632\n",
            "Loss: 1.1125441789627075\n",
            "Loss: 1.2058193683624268\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "def train(model, dataloader, epochs=10):\n",
        "    optimiser = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "    # writer = SummaryWriter()\n",
        "    # batch_idx = 0\n",
        "    # loss_function = torch.nn.MSELoss()\n",
        "    for epoch in range(epochs):\n",
        "        for batch in dataloader:\n",
        "            features, labels = batch\n",
        "            prediction = model(features)\n",
        "\n",
        "\n",
        "            # loss = torch.nn.functional.mse_loss(prediction, labels)   #most trivial way\n",
        "            # loss = loss_function(prediction,labels)   # utilize the method in the class MSELoss\n",
        "            loss = F.mse_loss(prediction,labels)\n",
        "\n",
        "            loss.backward()\n",
        "            print(\"Loss:\", loss.item())\n",
        "\n",
        "            optimiser.step()\n",
        "            optimiser.zero_grad()\n",
        "\n",
        "\n",
        "            # #LOG\n",
        "            # writer.add_scalar('Loss', loss.item(),batch_idx)\n",
        "    \n",
        "\n",
        "X, y = create_dummy_dataset(100, 4) # 100 examples, 4 features each\n",
        "dataloader = create_dummy_dataloader(X, y)\n",
        "model = NeuralNetwork(4, 1)\n",
        "train(model, dataloader)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5 (default, May 18 2021, 12:31:01) \n[Clang 10.0.0 ]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ffef1fb3247e42ae9cf3614f3519d4998b3b95643236a5d32641564963f5a3b8"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}